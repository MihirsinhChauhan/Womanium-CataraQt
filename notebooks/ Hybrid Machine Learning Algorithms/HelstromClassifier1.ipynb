{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96086a06",
   "metadata": {},
   "source": [
    "# Helstrom Quantum Centroid Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aaa101",
   "metadata": {},
   "source": [
    "This binary classifieris based on the concept of distinguishability between quantum states. It acts on density matrices—called density patterns—which is the quantum encoding of classical patterns of a dataset. The input vectors are encoded into quantum densities using either amplitude encoding or the inverse of the standard stereographic projection encoding method and the HQC model is trained using the encoded values.\n",
    "\n",
    "Ref[1]:Sergioli G, Giuntini R, Freytes H (2019) A new quantum approach to binary classification.\n",
    "    PLoS ONE 14(5): e0216224. https://doi.org/10.1371/journal.pone.0216224\n",
    "\n",
    "Ref[2] : https://helstrom-quantum-centroid-classifier.readthedocs.io/en/latest/user_guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d680405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, glob, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ac673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670e9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install and upgrade hqc \n",
    "#!pip install hqc\n",
    "#!pip install hqc --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd2f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1c0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = hqc.HQC(rescale=1.0, encoding='stereo', n_copies=2, class_wgt='equi', n_jobs=4, n_splits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d81c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get your HQC classification model, fit the features matrix X and binary target vector y\n",
    "#model1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa49ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52fce9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "IMG_HEIGHT =28\n",
    "IMG_WIDTH = 28\n",
    "\n",
    "# eye_diseases_classification dataset\n",
    "IMG_ROOT = 'D:\\Womanium2023\\GlobalQuantumProject\\Datasets\\eye_diseases_classification\\Proc\\\\'\n",
    "IMG_DIR = [IMG_ROOT+'normal',\n",
    "           IMG_ROOT+'cataract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "514feb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Womanium2023\\\\GlobalQuantumProject\\\\Datasets\\\\eye_diseases_classification\\\\Proc\\\\normal',\n",
       " 'D:\\\\Womanium2023\\\\GlobalQuantumProject\\\\Datasets\\\\eye_diseases_classification\\\\Proc\\\\cataract']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7bbe377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(0,\n",
    "                  columns=['paths',\n",
    "                           'cataract'],\n",
    "                  index=range(2500))\n",
    "\n",
    "filepaths = glob.glob(IMG_ROOT + '*/*')\n",
    "\n",
    "for i, filepath in enumerate(filepaths):\n",
    "    filepath = os.path.split(filepath)\n",
    "    df.iloc[i, 0] = filepath[0] + '/' + filepath[1]\n",
    "\n",
    "    if filepath[0] == IMG_DIR[0]:    # normal\n",
    "            df.iloc[i, 1] = 0\n",
    "    elif filepath[0] == IMG_DIR[1]:  # cataract\n",
    "            df.iloc[i, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6994ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.paths !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "828df116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of normal and cataract images\n",
      "cataract\n",
      "0    1074\n",
      "1     938\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Number of normal and cataract images')\n",
    "print(df['cataract'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1739f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df,\n",
    "                                     test_size=0.2,\n",
    "                                     random_state=SEED,\n",
    "                                     stratify=df['cataract'])\n",
    "\n",
    "train_df, val_df = train_test_split(train_df,\n",
    "                                    test_size=0.15,\n",
    "                                    random_state=SEED,\n",
    "                                    stratify=train_df['cataract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a59d2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1367/1367 [00:00<00:00, 3081.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 242/242 [00:00<00:00, 3612.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 403/403 [00:00<00:00, 3636.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_datasets(df):\n",
    "    imgs = []\n",
    "\n",
    "    for path in tqdm(df['paths']):\n",
    "        #print(path)\n",
    "        img = cv2.imread(path,0)\n",
    "        img = cv2.resize(img,(IMG_HEIGHT,IMG_WIDTH))\n",
    "        img=img.flatten() #convert 2d to 1d array\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.array(imgs, dtype='float32')\n",
    "\n",
    "    labels = df.cataract\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "train_imgs, train_labels = create_datasets(train_df)\n",
    "val_imgs, val_labels = create_datasets(val_df)\n",
    "test_imgs, test_labels = create_datasets(test_df)\n",
    "\n",
    "train_labels = train_labels.astype(np.uint8)\n",
    "val_labels = val_labels.astype(np.uint8)\n",
    "test_labels = test_labels.astype(np.uint8)\n",
    "\n",
    "#train_imgs = train_imgs / 255.0\n",
    "#val_imgs = val_imgs / 255.0\n",
    "#test_imgs = test_imgs / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13bd4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainnumber = (1367, 784) testnumber =  (403, 784) valnumber = (242, 784)\n"
     ]
    }
   ],
   "source": [
    "print('trainnumber =', train_imgs.shape,'testnumber = ', test_imgs.shape,'valnumber =', val_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b335b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "kernel_pca = KernelPCA(\n",
    "    n_components=6, kernel=\"rbf\", gamma=None, fit_inverse_transform=True, alpha=0.1\n",
    ")\n",
    "\n",
    "# rescale the data so it has unit standard deviation and zero mean.Fit using full set of train data\n",
    "scaler = preprocessing.StandardScaler().fit(train_imgs)\n",
    "train_scaled = scaler.transform(train_imgs)\n",
    "val_scaled = scaler.transform(val_imgs)\n",
    "test_scaled = scaler.transform(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bea682a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset size if required\n",
    "n_train = 1367\n",
    "n_test = 403\n",
    "train_images = train_scaled[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_scaled[:n_test]\n",
    "test_labels = test_labels[:n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8eb5ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de78ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply kernel pca for reducing dimensionality \n",
    "X_train= kernel_pca.fit(train_images).transform(train_images).astype(np.float32)\n",
    "testfit = kernel_pca.transform(test_images).astype(np.float32)\n",
    "valfit = kernel_pca.transform(val_scaled).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13af1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c770ba2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HQC(encoding=&#x27;stereo&#x27;, n_copies=2, n_jobs=4, n_splits=2, rescale=1.0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HQC</label><div class=\"sk-toggleable__content\"><pre>HQC(encoding=&#x27;stereo&#x27;, n_copies=2, n_jobs=4, n_splits=2, rescale=1.0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HQC(encoding='stereo', n_copies=2, n_jobs=4, n_splits=2, rescale=1.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.predict(testfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f148af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7419354838709677"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(testfit, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44a84234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7768595041322314"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(valfit, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473bd52",
   "metadata": {},
   "source": [
    "For best hyperparameter search, use GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = { 'rescale':[0.5,1,1.5],'encoding':['amplit', 'stereo'], 'n_copies':[1, 2], 'class_wgt':['equi', 'weighted']}\n",
    "models = GridSearchCV(hqc.HQC(), param_grid).fit(X_train,train_labels)\n",
    "\n",
    "pd.DataFrame(models.cv_results_)\n",
    "models.best_score_\n",
    "models.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
